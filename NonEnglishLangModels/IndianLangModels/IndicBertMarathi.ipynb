{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sentencepiece --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_str_rel = pd.read_csv('C:\\\\Users\\\\ADMIN\\\\Downloads\\\\mar_train.csv')\n",
    "df_str_rel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_str_rel['Split_Text'] = df_str_rel['Text'].apply(lambda x: x.split(\"\\n\"))\n",
    "df_str_rel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Specify the model name or path\n",
    "model_name_or_path = \"ai4bharat/indic-bert\"\n",
    "\n",
    "# Load the IndicBERT model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "model = AutoModel.from_pretrained(model_name_or_path)\n",
    "\n",
    "# Function to encode text and get embeddings\n",
    "def encode_text(text):\n",
    "    input_ids = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)[\"input_ids\"]\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(input_ids).last_hidden_state.mean(dim=1)  # Average pooling\n",
    "    return embeddings\n",
    "\n",
    "# Calculate cosine similarity between two sentence embeddings\n",
    "def calculate_similarity(sentence1, sentence2):\n",
    "    embeddings1 = encode_text(sentence1)\n",
    "    embeddings2 = encode_text(sentence2)\n",
    "    similarity = cosine_similarity(embeddings1, embeddings2)\n",
    "    return similarity[0][0]\n",
    "\n",
    "sentence1 = \"This is the first sentence.\"\n",
    "sentence2 = \"This is the second sentence.\"\n",
    "\n",
    "similarity = calculate_similarity(sentence1, sentence2)\n",
    "print(f\"Cosine Similarity: {similarity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_scores = df_str_rel['Score'].values\n",
    "pred_scores = []\n",
    "\n",
    "for index,row in df_str_rel.iterrows():\n",
    "  s1,s2 = row[\"Text\"].split(\"\\n\")\n",
    "\n",
    "  # Overlap score\n",
    "  pred_scores.append(calculate_similarity(s1,s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Spearman Correlation:\", round(spearmanr(true_scores,pred_scores)[0],2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install protobuf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
